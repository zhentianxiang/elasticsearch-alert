package alert

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/url"
	"strings"
	"time"

	"github.com/robfig/cron/v3"

	"elasticsearch-alert/internal/config"
	eswrap "elasticsearch-alert/internal/elasticsearch"
	"elasticsearch-alert/internal/logging"
	"elasticsearch-alert/internal/notification"
	"os"
	"path/filepath"

	"gopkg.in/yaml.v3"
)

type Engine struct {
	cfg       *config.Config
	es        *eswrap.Client
	notifiers []notification.Notifier

	cron     *cron.Cron
	location *time.Location

	rules        []Rule
	lastAlertAt  map[string]time.Time
	defaultQuiet time.Duration
	sampleSize   int
}

// Rules è¿”å›å½“å‰åŠ è½½çš„æ‰€æœ‰è§„åˆ™ï¼ˆåªè¯»ä½¿ç”¨ï¼‰
func (e *Engine) Rules() []Rule {
	return e.rules
}

func NewEngine(cfg *config.Config, es *eswrap.Client, notifiers []notification.Notifier) (*Engine, error) {
	loc, err := time.LoadLocation(cfg.Scheduler.Timezone)
	if err != nil {
		loc = time.Local
	}
	c := cron.New(cron.WithLocation(loc), cron.WithSeconds())

	engine := &Engine{
		cfg:          cfg,
		es:           es,
		notifiers:    notifiers,
		cron:         c,
		location:     loc,
		lastAlertAt:  make(map[string]time.Time),
		defaultQuiet: cfg.Rules.GetDefaultQuietPeriod(),
		sampleSize:   cfg.Rules.SampleSize,
	}
	if err := engine.loadRules(cfg.Rules.Directory); err != nil {
		return nil, err
	}
	return engine, nil
}

func (e *Engine) Start() error {
	for i := range e.rules {
		r := e.rules[i]
		_, err := e.cron.AddFunc(r.Cron, func() { e.executeRule(r) })
		if err != nil {
			return fmt.Errorf("ä¸ºè§„åˆ™ %q æ·»åŠ å®šæ—¶ä»»åŠ¡å¤±è´¥: %w", r.Name, err)
		}
		logging.Infof("è§„åˆ™å·²æ³¨å†Œ: %s cron=%s çª—å£=%s", r.Name, r.Cron, r.TimeWindow)
	}
	e.cron.Start()
	return nil
}

func (e *Engine) Stop() {
	ctx := e.cron.Stop()
	<-ctx.Done()
}

func (e *Engine) loadRules(dir string) error {
	entries, err := os.ReadDir(dir)
	if err != nil {
		return fmt.Errorf("read rules dir: %w", err)
	}
	var rules []Rule
	for _, entry := range entries {
		if entry.IsDir() || !(strings.HasSuffix(entry.Name(), ".yaml") || strings.HasSuffix(entry.Name(), ".yml")) {
			continue
		}
		path := filepath.Join(dir, entry.Name())
		data, err := os.ReadFile(path)
		if err != nil {
			return fmt.Errorf("read rule %s: %w", path, err)
		}
		var r Rule
		if err := yaml.Unmarshal(data, &r); err != nil {
			return fmt.Errorf("unmarshal rule %s: %w", path, err)
		}
		if r.Name == "" || r.Index == "" || r.Cron == "" || r.TimeWindow == "" {
			return fmt.Errorf("invalid rule %s: name/index/cron/timeWindow required", path)
		}
		rules = append(rules, r)
	}
	e.rules = rules
	return nil
}

func (e *Engine) executeRule(r Rule) {
	defer func() {
		if rec := recover(); rec != nil {
			logging.Errorf("è§„åˆ™ %s æ‰§è¡Œå‘ç”Ÿ panic: %v", r.Name, rec)
		}
	}()
	now := time.Now().In(e.location)
	logging.Debugf("è§„åˆ™å®šæ—¶è§¦å‘: %s æ—¶é—´=%s", r.Name, now.Format("2006-01-02 15:04:05"))

	if !e.shouldFire(r, now) {
		logging.Debugf("è§„åˆ™ %s è·³è¿‡æ‰§è¡Œï¼ˆé™é»˜æœŸå†…ï¼‰", r.Name)
		return
	}

	count, samples, err := e.queryCountAndSamples(r)
	if err != nil {
		logging.Errorf("è§„åˆ™ %s æŸ¥è¯¢å‡ºé”™: %v", r.Name, err)
		return
	}
	logging.Debugf("è§„åˆ™ %s æŸ¥è¯¢å®Œæˆ: å‘½ä¸­=%d çª—å£=%s", r.Name, count, r.TimeWindow)

	if !e.hitThreshold(r, count) {
		if r.Threshold.CountGt != nil {
			logging.Debugf("è§„åˆ™ %s æœªè§¦å‘: å‘½ä¸­=%d é˜ˆå€¼=>%d", r.Name, count, *r.Threshold.CountGt)
		} else {
			logging.Debugf("è§„åˆ™ %s æœªè§¦å‘: æœªé…ç½®é˜ˆå€¼ å‘½ä¸­=%d", r.Name, count)
		}
		return
	}
	logging.Infof("è§„åˆ™ %s è§¦å‘å‘Šè­¦: å‘½ä¸­=%d é€šçŸ¥æ¸ é“=%v", r.Name, count, r.Alerts.Channels)

	title := fmt.Sprintf("[Elasticsearch Alert] %s", r.Name)
	body := e.renderBody(r, count, samples)
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()
	for _, ch := range r.Alerts.Channels {
		for _, n := range e.notifiers {
			if n.Name() == ch || (ch == "console" && n.Name() == "console") {
				if err := n.Send(ctx, title, body); err != nil {
					logging.Errorf("é€šè¿‡æ¸ é“ %s å‘é€å‘Šè­¦å¤±è´¥: %v", n.Name(), err)
				} else {
					logging.Debugf("é€šè¿‡æ¸ é“ %s å‘é€å‘Šè­¦æˆåŠŸ", n.Name())
				}
			}
		}
	}
	e.lastAlertAt[r.Name] = now
}

func (e *Engine) shouldFire(r Rule, now time.Time) bool {
	last, ok := e.lastAlertAt[r.Name]
	if !ok {
		return true
	}
	quiet := r.Dedup.GetQuietPeriod(e.defaultQuiet)
	return now.Sub(last) >= quiet
}

func (e *Engine) hitThreshold(r Rule, count int) bool {
	if r.Threshold.CountGt != nil {
		return count > *r.Threshold.CountGt
	}
	return false
}

func (e *Engine) renderBody(r Rule, count int, samples []map[string]any) string {
	now := time.Now().In(e.location)
	severity := r.Severity
	if severity == "" {
		severity = "Medium"
	}

	var b strings.Builder
	// æ ‡é¢˜è¡Œï¼ˆæ­£æ–‡å†…éƒ¨çš„è§†è§‰æ ‡é¢˜ï¼Œå„æ¸ é“å¤–å±‚ä¹Ÿæœ‰æ ‡é¢˜ï¼‰
	b.WriteString("ğŸš¨ **Elasticsearch æ—¥å¿—å‘Šè­¦**\n\n")

	if r.Description != "" {
		b.WriteString(r.Description + "\n\n")
	}

	// æ¦‚è§ˆä¿¡æ¯
	b.WriteString("ğŸ“Š **å‘Šè­¦æ¦‚è§ˆ**\n")
	b.WriteString(fmt.Sprintf("- **è§„åˆ™åç§°ï¼š** %s\n", r.Name))
	b.WriteString(fmt.Sprintf("- **å‘Šè­¦çº§åˆ«ï¼š** %s\n", severity))
	b.WriteString(fmt.Sprintf("- **è§¦å‘æ—¶é—´ï¼š** %s\n", now.Format("2006-01-02 15:04:05")))
	b.WriteString(fmt.Sprintf("- **ç´¢å¼•ï¼š** %s\n", r.Index))
	b.WriteString(fmt.Sprintf("- **æ—¶é—´çª—ï¼š** %s\n", r.TimeWindow))
	b.WriteString(fmt.Sprintf("- **å‘½ä¸­æ¡æ•°ï¼š** %d\n", count))
	if r.Threshold.CountGt != nil {
		b.WriteString(fmt.Sprintf("- **é˜ˆå€¼ï¼š** > %d æ¡\n", *r.Threshold.CountGt))
	}
	if r.QueryString != "" {
		b.WriteString(fmt.Sprintf("- **æŸ¥è¯¢ï¼š** %s\n", r.QueryString))
	} else if r.DSL != nil {
		b.WriteString("- **æŸ¥è¯¢ï¼š** DSL\n")
	}

	// åªå±•ç¤ºä¸€æ¡ä»£è¡¨æ€§çš„æ ·ä¾‹ï¼Œçªå‡ºèŠ‚ç‚¹/Pod/é•œåƒ/é”™è¯¯æ—¥å¿—
	if len(samples) > 0 {
		doc := samples[0]
		ts, _ := doc["@timestamp"].(string)
		indexName, _ := doc["_index"].(string)
		docID, _ := doc["_id"].(string)
		node, _ := doc["kubernetes_host"].(string)
		ns, _ := doc["kubernetes_namespace_name"].(string)
		pod, _ := doc["kubernetes_pod_name"].(string)
		image, _ := doc["kubernetes_container_image"].(string)
		msg, _ := doc["message"].(string)
		truncated := false
		if len(msg) > 800 {
			truncated = true
			msg = msg[:800] + "..."
		}

		b.WriteString("\nğŸ“Œ **æœ¬æ¬¡å‘Šè­¦ç›®æ ‡**\n")
		if node != "" {
			b.WriteString(fmt.Sprintf("- **èŠ‚ç‚¹åç§°ï¼š** %s\n", node))
		}
		if ns != "" {
			b.WriteString(fmt.Sprintf("- **å‘½åç©ºé—´ï¼š** %s\n", ns))
		}
		if pod != "" {
			b.WriteString(fmt.Sprintf("- **Pod åç§°ï¼š** %s\n", pod))
		}
		if image != "" {
			b.WriteString(fmt.Sprintf("- **Pod é•œåƒï¼š** %s\n", image))
		}
		if ts != "" {
			b.WriteString(fmt.Sprintf("- **æ—¥å¿—æ—¶é—´ï¼š** %s\n", ts))
		}

		if msg != "" {
			b.WriteString("\nğŸ§¾ **é”™è¯¯æ—¥å¿—**\n")
			b.WriteString(msg)
			if truncated {
				b.WriteString("\n...(æ—¥å¿—å†…å®¹è¾ƒé•¿ï¼Œå·²æˆªæ–­æ˜¾ç¤º)")
			}
			b.WriteString("\n")
		}

		// è¯¦ç»†æ—¥å¿—é“¾æ¥ï¼šä¼˜å…ˆæŒ‡å‘æœ¬æœåŠ¡æä¾›çš„ Web é¡µé¢ï¼Œå…¶æ¬¡å›é€€åˆ°ç›´æ¥è®¿é—® ES çš„ _doc API
		if indexName != "" && docID != "" {
			if e.cfg.Web.BaseURL != "" {
				base := strings.TrimRight(e.cfg.Web.BaseURL, "/")
				detailURL := fmt.Sprintf("%s/logs?index=%s&id=%s",
					base,
					url.QueryEscape(indexName),
					url.QueryEscape(docID),
				)
				b.WriteString("\nğŸ”— **è¯¦ç»†æ—¥å¿—é“¾æ¥ï¼š** ")
				b.WriteString(detailURL)
				b.WriteString("\n")
			} else if len(e.cfg.Elasticsearch.Addresses) > 0 {
				base := e.cfg.Elasticsearch.Addresses[0]
				base = strings.TrimRight(base, "/")
				detailURL := fmt.Sprintf("%s/%s/_doc/%s?pretty", base, indexName, docID)
				b.WriteString("\nğŸ”— **è¯¦ç»†æ—¥å¿—é“¾æ¥ï¼š** ")
				b.WriteString(detailURL)
				b.WriteString("\n")
			}
		}
	}

	// æ–¹ä¾¿åœ¨é€šçŸ¥æ¨¡ç‰ˆåº•éƒ¨é¢å¤–è¿½åŠ  @æ‰€æœ‰äººï¼Œè¿™é‡Œä¸ç›´æ¥å¤„ç† @ æ–‡æœ¬
	return b.String()
}

func (e *Engine) queryCountAndSamples(r Rule) (int, []map[string]any, error) {
	window := r.TimeWindow
	if window == "" {
		window = "5m"
	}
	rangeGte := fmt.Sprintf("now-%s", window)
	rangeLt := "now"

	query := map[string]any{
		"size": e.sampleSize,
		"sort": []map[string]any{
			{"@timestamp": map[string]any{"order": "desc"}},
		},
		"track_total_hits": true,
		"query": map[string]any{
			"bool": map[string]any{
				"filter": []any{
					map[string]any{
						"range": map[string]any{
							"@timestamp": map[string]any{
								"gte": rangeGte,
								"lt":  rangeLt,
							},
						},
					},
				},
			},
		},
	}
	boolQuery := query["query"].(map[string]any)["bool"].(map[string]any)
	filters := boolQuery["filter"].([]any)
	if r.QueryString != "" {
		filters = append(filters, map[string]any{
			"query_string": map[string]any{
				"query":            r.QueryString,
				"default_operator": "AND",
			},
		})
	} else if r.DSL != nil {
		filters = append(filters, r.DSL)
	}
	boolQuery["filter"] = filters

	var buf bytes.Buffer
	_ = json.NewEncoder(&buf).Encode(query)

	res, err := e.es.Search(r.Index, &buf)
	if err != nil {
		return 0, nil, err
	}
	defer res.Body.Close()
	if res.IsError() {
		return 0, nil, fmt.Errorf("search error: %s", res.String())
	}
	var parsed struct {
		Hits struct {
			Total struct {
				Value int `json:"value"`
			} `json:"total"`
			Hits []struct {
				Index  string         `json:"_index"`
				ID     string         `json:"_id"`
				Source map[string]any `json:"_source"`
			} `json:"hits"`
		} `json:"hits"`
	}
	if err := json.NewDecoder(res.Body).Decode(&parsed); err != nil {
		return 0, nil, err
	}
	samples := make([]map[string]any, 0, len(parsed.Hits.Hits))
	for _, h := range parsed.Hits.Hits {
		doc := h.Source
		if doc == nil {
			doc = make(map[string]any)
		}
		// å°† _index ä¸ _id ä¸€å¹¶æ”¾å…¥æ ·ä¾‹æ–‡æ¡£ä¸­ï¼Œä¾¿äºåç»­ç”Ÿæˆè¯¦ç»†æ—¥å¿—é“¾æ¥
		doc["_index"] = h.Index
		doc["_id"] = h.ID
		samples = append(samples, doc)
	}
	return parsed.Hits.Total.Value, samples, nil
}
